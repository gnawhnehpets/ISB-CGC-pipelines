#!/usr/bin/env python
import os
import re
import shutil
import argparse
import pyinotify
from pipelines.utils import PipelinesConfig, PipelinesConfigUpdateHandler, PipelineSchedulerUtils, PipelineDbUtils

# NOTE: this process should be started up as part of the scheduling system (managed by Supervisor)


class DependentJobsHandler(pyinotify.ProcessEvent):
	def my_init(self, config=None):
		self._config = config
		self._pipelineDbUtils = PipelineDbUtils(self._config)

	def process_IN_CREATE(self, event):
		if re.match('^.*/SUCCEEDED/.*$', event.pathname):
			children = self._pipelineDbUtils.getChildJobs(event.name)
			
			if len(children) > 0:
				PipelineSchedulerUtils.writeStdout("Checking child jobs: {children}".format(children=','.join(children)))
				for c in children:
					parents = self._pipelineDbUtils.getParentJobs(c)
					succeededParents = 1

					for p in parents.remove(event.name):
						status = self._pipelineDbUtils.getJobInfo(select=["current_status"], where={"job_id": p})[0].current_status

						if status == "SUCCEEDED":
							succeededParents += 1

						else:
							break

					if len(p) == succeededParents:
						shutil.copy(os.path.join(self._config.pipelines_home, "DEPENDENT", str(c)), os.path.join(self._config.pipelines_home, "WAITING", str(c)))
						os.remove(os.path.join(self._config.pipelines_home, "DEPENDENT", str(c)))
		
					else:
						childJobInfo = self._pipelineDbUtils.getJobInfo(select=["pipeline_name", "tag"], where={"job_id": c})
						parentJobInfo = self._pipelineDbUtils.getJobInfo(select=["pipeline_name", "tag"], where={"job_id": p})
						PipelineSchedulerUtils.writeStderr("Couldn't start job {c} (pipeline: {cp}, tag: {ct}) : depends on job {p} (pipeline: {pp}, tag: {pt}), which has a status of {s}".format(c=str(c), cp=childJobInfo[0].pipeline_name, ct=childJobInfo[0].tag, p=p, pp=parentJobInfo[0].pipeline_name, pt=parentJobInfo[0].tag, s=status))

			else:
				PipelineSchedulerUtils.writeStdout("Job {jobid} has no child jobs to check!".format(jobid=event.name))

		else:
			jobInfo = self._pipelineDbUtils.getJobInfo(select=["pipeline_name", "tag"], where={"job_id": event.name})
			PipelineSchedulerUtils.writeStderr("Couldn't start downstream jobs for job {j} ({pipeline}-{tag}) : status is FAILED".format(j=event.name, pipeline=jobInfo[0].pipeline_name, tag=jobInfo[0].tag))


def watch(args):
	config = PipelinesConfig(args.config)

	PipelineSchedulerUtils.writeStdout("Watching DEPENDENT jobs...")

	jobStatusManager = pyinotify.WatchManager()
	jobStatusNotifier = pyinotify.Notifier(jobStatusManager)

	jobStatusManager.add_watch(config.path, pyinotify.IN_CLOSE_WRITE, proc_fun=PipelinesConfigUpdateHandler(config=config))
	jobStatusManager.add_watch(os.path.join(config.pipelines_home, "SUCCEEDED"), pyinotify.IN_CREATE, proc_fun=DependentJobsHandler(config=config))
	jobStatusManager.add_watch(os.path.join(config.pipelines_home, "FAILED"), pyinotify.IN_CREATE, proc_fun=DependentJobsHandler(config=config))
	jobStatusNotifier.loop()

if __name__ == "__main__":
	parser = argparse.ArgumentParser()
	parser.add_argument("--config")

	args = parser.parse_args()

	watch(args)






		



