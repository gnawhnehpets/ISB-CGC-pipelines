#!/usr/bin/env python
import os
import json
import shutil
import httplib2
import argparse
import pyinotify
from apiclient.discovery import build
from googleapiclient.errors import HttpError
from oauth2client.client import GoogleCredentials
from lib.pipelines.utils import PipelinesConfig, PipelinesConfigUpdateHandler, PipelineSchedulerUtils, PipelineDbUtils

# NOTE: this process should be started up as part of the scheduling system (managed by Supervisor)


class RunningJobsHandler(pyinotify.ProcessEvent):
	def my_init(self, config=None):
		self._config = config
		self._pipelineDbUtils = PipelineDbUtils(self._config)
		self._credentials = GoogleCredentials.get_application_default()
		self._http = self._credentials.authorize(httplib2.Http())

		if self._credentials.access_token_expired:
			self.credentials.refresh(self._http)

		self._pipelineService = build('genomics', 'v1alpha2', http=self._http)

	def process_IN_CREATE(self, event):
		# event.pathname -> <pipelines_home>/RUNNING/<job_id>
		jobInfo = self._pipelineDbUtils.getJobInfo(select=["pipeline_name", "tag"], where={"job_id": event.name})
		PipelineSchedulerUtils.writeStdout("Starting job {pipeline}-{tag} ...".format(pipeline=jobInfo[0].pipeline_name, tag=jobInfo[0].tag))

		req = json.load(open(event.pathname))

		try:
			run = self._pipelineService.pipelines().run(body=req).execute()
		except HttpError as e:
			PipelineSchedulerUtils.writeStderr("ERROR: couldn't start job {pipeline}-{tag} : {reason}".format(pipeline=jobInfo[0].pipeline_name, tag=jobInfo[0].tag, reason=e))
			self._pipelineDbUtils.updateJob(event.name, setValues={"current_status": "ERROR"})
			shutil.copy(event.pathname, os.path.join(self._config.pipelines_home, "ERROR", event.name))
			os.remove(event.pathname)
			
		else:
			PipelineSchedulerUtils.writeStdout("Operation submitted: {op}".format(op=run["name"]))
			self._pipelineDbUtils.updateJob(event.name, setValues={"current_status": "RUNNING", "operation_id": run["name"], "stdout_log": "{op}-stdout.log".format(op=run["name"].split('/')[1]), "stderr_log": "{op}-stderr.log".format(op=run["name"].split('/')[1]), "create_time": run["metadata"]["createTime"]}) # TODO: create_time

			if not os.path.isfile(os.path.join(self._config.pipelines_home, "MONITOR", "start")):
				try:
					open(os.path.join(self._config.pipelines_home, "MONITOR", "start"), 'a').close()
				except IOError as e:
					PipelineSchedulerUtils.writeStderr("ERROR: couldn't start the monitoring process : {reason}".format(reason=e))


def watch(args):
	config = PipelinesConfig(args.config)

	PipelineSchedulerUtils.writeStdout("Watching RUNNING jobs...")

	jobStatusManager = pyinotify.WatchManager()
	jobStatusNotifier = pyinotify.Notifier(jobStatusManager)

	jobStatusManager.add_watch(config.path, pyinotify.IN_CLOSE_WRITE, proc_fun=PipelinesConfigUpdateHandler(config=config))
	jobStatusManager.add_watch(os.path.join(config.pipelines_home, "RUNNING"), pyinotify.IN_CREATE, proc_fun=RunningJobsHandler(config=config))
	jobStatusNotifier.loop()

if __name__ == "__main__":
	parser = argparse.ArgumentParser()
	parser.add_argument("--config")

	args = parser.parse_args()

	watch(args)





		



