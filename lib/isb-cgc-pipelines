#!/usr/bin/env python
import os
import shutil
import argparse
import httplib2
import subprocess
from googleapiclient import discovery
from oauth2client.client import GoogleCredentials
from pipelines.schema import PipelineSchema
from pipelines.builder import PipelineBuilder
from pipelines.utils import PipelinesConfig, PipelineDbUtils, PipelineServiceUtils, PipelineSchedulerUtils


def deployService(args, config):
	try:
		subprocess.check_call(["gsutil", "ls", args.bucket])
	except subprocess.CalledProcessError as e:
		print "ERROR: there was a problem deploying cloud functions to bucket {bucket} : {reason}".format(bucket=args.bucket, reason=e)
		exit(-1)

	# API service objects
	credentials = GoogleCredentials.get_application_default()
	http = credentials.authorize(httplib2.Http())
	if credentials.access_token_expired:
		credentials.refresh(http)

	pubsub = discovery.build('pubsub', 'v1', http=http)
	logging = discovery.build('logging', 'v2', http=http)

	PipelineServiceUtils.bootstrapMessageHandlers(pubsub, logging, config, mode="local")

def submitPipeline(args, config):

	# send the request message to the local rabbitmq server
	if args.scriptUrl:
		pipelineSpec = PipelineSchema(args.pipelineName, config, args.logsPath, args.imageName,
			scriptUrl=args.scriptUrl, cores=args.cores,
			mem=args.mem, diskSize=args.diskSize, diskType=args.diskType, env=args.env,
			inputs=args.inputs, outputs=args.outputs, tag=args.tag,
			preemptible=args.preemptible)
	elif args.cmd:
		pipelineSpec = PipelineSchema(args.pipelineName, config, args.logsPath, args.imageName, cmd=args.cmd,
			cores=args.cores,
			mem=args.mem, diskSize=args.diskSize, diskType=args.diskType, env=args.env,
			inputs=args.inputs, outputs=args.outputs, tag=args.tag,
			preemptible=args.preemptible)

	pipelineBuilder = PipelineBuilder(config)
	pipelineBuilder.addStep(pipelineSpec)
	pipelineBuilder.run()


def stopPipeline(args, unknown, config):  # TODO: implement
	pass


def restartJobs(args, config):  # TODO: reimplement
	pipelineDbUtils = PipelineDbUtils(config)

	if args.jobId:
		currentStatus = pipelineDbUtils.getJobInfo(select=["current_status"], where={"job_id": args.jobId})[0].current_status
		pipelineDbUtils.updateJob(args.jobId, setValues={"current_status": "WAITING"})
		
		shutil.copy(os.path.join(config.pipelines_home, currentStatus, args.jobId), os.path.join(config.pipelines_home, "WAITING", args.jobId)) 
		shutil.rmtree(os.path.join(config.pipelines_home, currentStatus, args.jobId))

	if args.preempted:
		preempted = pipelineDbUtils.getJobInfo(select=["job_id"], where={"current_status": "PREEMPTED"})
		
		for p in preempted:
			pipelineDbUtils.updateJob(p, setValues={"current_status": "WAITING"})
		
			shutil.copy(os.path.join(config.pipelines_home, "PREEMPTED", p), os.path.join(config.pipelines_home, "WAITING", p)) 
			shutil.rmtree(os.path.join(config.pipelines_home, "PREEMPTED", p))

	pipelineDbUtils.closeConnection()


def cleanPipelines(args, unknown, config):  # TODO: implement
	pass


def getJobLogs(args, config):  # TODO: reimplement
	pipelineDbUtils = PipelineDbUtils(config)

	jobInfo = pipelineDbUtils.getJobInfo(select=["stdout_log", "stderr_log", "gcs_log_path"], where={"job_id": args.jobId})

	with open(os.devnull, 'w') as fnull:
		if args.stdout:
			try:
				stdoutLogFile = subprocess.check_output(["gsutil", "cat", os.path.join(jobInfo[0].gcs_log_path, jobInfo[0].stdout_log)], stderr=fnull)
			except subprocess.CalledProcessError as e:
				print "ERROR: couldn't get the stdout log : {reason}".format(reason=e)
				exit(-1)

			print "STDOUT:\n"
			print stdoutLogFile
			print "---------\n"

		if args.stderr:
			try:
				stderrLogFile = subprocess.check_output(["gsutil", "-q", "cat", os.path.join(jobInfo[0].gcs_log_path, jobInfo[0].stderr_log)], stderr=fnull)
			except subprocess.CalledProcessError as e:
				print "ERROR: couldn't get the stderr log : {reason}".format(reason=e)
				exit(-1)

			print "STDERR:\n"
			print stderrLogFile
			print "---------\n"

	pipelineDbUtils.closeConnection()


def getJobsList(args, unknown, config):  # TODO: reimplement
	header = "JOBID%sPIPELINE%sOPERATION ID%sTAG%sSTATUS%sCREATE TIME%sPREEMPTIONS\n"
	jobStatusString = "{jobId}%s{pipeline}%s{operationId}%s{tag}%s{status}%s{createTime}%s{preemptions}\n"
	
	pipelineDbUtils = PipelineDbUtils(config)

	parser = argparse.ArgumentParser()
	parser.add_argument("--pipeline")
	parser.add_argument("--status", choices=["running", "waiting", "succeeded", "failed", "error", "preempted"])
	parser.add_argument("--createTimeAfter")
	parser.add_argument("--limit", default=50)

	args = parser.parse_args(args=unknown, namespace=args)

	select = ["job_id", "operation_id", "pipeline_name", "tag", "current_status", "create_time", "preemptions"]
	where = {}

	if args.pipeline:
		where["pipeline_name"] = args.pipeline

	if args.status:
		where["current_status"] = args.status

	if args.createTimeAfter:
		where["create_time"] = {
			"value": args.createTimeAfter,
			"operator": ">="
		}

	jobs = pipelineDbUtils.getJobInfo(select=select, where=where)

	pipelineDbUtils.closeConnection()

	def fieldPadding(maxLen, actualLen):
		return ''.join([' ' for x in range(maxLen - actualLen + 4)])

	if len(jobs) > 0:
		jobIdLengths = [len(str(x.job_id)) if x.job_id is not None else len(str("None")) for x in jobs]
		jobIdLengths.append(len("JOBID"))
		maxJobIdLen = max(jobIdLengths)

		pipelineLengths = [len(x.pipeline_name) if x.pipeline_name is not None else len(str("None")) for x in jobs]
		pipelineLengths.append(len("PIPELINE"))
		maxPipelineLen = max(pipelineLengths)

		operationIdLengths = [len(x.operation_id.split('/')[1]) if x.operation_id is not None else len(str("None")) for x in jobs]
		operationIdLengths.append(len("OPERATION ID"))
		maxOperationIdLen = max(operationIdLengths)

		statusLengths = [len(x.current_status) if x.current_status is not None else len(str("None")) for x in jobs]
		statusLengths.append(len("STATUS"))
		maxStatusLen = max(statusLengths)

		tagLengths = [len(x.tag) if x.tag is not None else len(str("None")) for x in jobs]
		tagLengths.append(len("TAG"))
		maxTagLen = max(tagLengths)

		createTimeLengths = [len(x.create_time) if x.create_time is not None else len(str("None")) for x in jobs]
		createTimeLengths.append(len("CREATE TIME"))
		maxCreateTimeLen = max(createTimeLengths)

		print header % (fieldPadding(maxJobIdLen, len("JOBID")), fieldPadding(maxPipelineLen, len("PIPELINE")), fieldPadding(maxOperationIdLen, len("OPERATION ID")), fieldPadding(maxTagLen, len("TAG")), fieldPadding(maxStatusLen, len("STATUS")), fieldPadding(maxCreateTimeLen, len("CREATE TIME")))
		for j in jobs:
			if j.create_time is not None:
				ct = j.create_time
			else:
				ct = "None"
			if j.operation_id is not None:
				op = j.operation_id.split('/')[1]
			else:
				op = "None"

			print jobStatusString.format(jobId=j.job_id, pipeline=j.pipeline_name, operationId=op, tag=j.tag, status=j.current_status, createTime=ct, preemptions=j.preemptions) % (fieldPadding(maxJobIdLen, len(str(j.job_id))), fieldPadding(maxPipelineLen, len(j.pipeline_name)), fieldPadding(maxOperationIdLen, len(op)), fieldPadding(maxTagLen, len(j.tag)), fieldPadding(maxStatusLen, len(j.current_status)), fieldPadding(maxCreateTimeLen, len(ct)))

	else:
		print "No jobs found"

def main(args, unknown):
	if args.subcommand == "deploy":
		config = PipelinesConfig()
		deployService(args, config)

	elif args.subcommand == "scheduler":
		config = PipelinesConfig(PIPELINES_CONFIG_PATH)

		if args.schedulerOperation == "stop":
			PipelineSchedulerUtils.stopScheduler(args, unknown, config)
		elif args.schedulerOperation == "start":
			PipelineSchedulerUtils.startScheduler(args, unknown, config)
		else:
			print "ERROR: unrecognized option {o}".format(o=args.startOrStop)
			exit(-1)

	if args.subcommand == "submit":
		config = PipelinesConfig()
		submitPipeline(args, config)

	elif args.subcommand == "stop":
		config = PipelinesConfig()
		stopPipeline(args, unknown, config) 

	elif args.subcommand == "restart":
		config = PipelinesConfig()
		restartJobs(args, config)

	elif args.subcommand == "clean":
		config = PipelinesConfig()
		cleanPipelines(args, unknown, config) 

	elif args.subcommand == "logs":
		config = PipelinesConfig()
		getJobLogs(args, config)

	elif args.subcommand == "list":
		config = PipelinesConfig()

		if args.entity == "jobs":
			getJobsList(args, unknown, config)

	elif args.subcommand == "config":
		parser = argparse.ArgumentParser()

		if args.configOperation == "set":
			parser.add_argument("parameter", choices=["gcp/project_id", "gcp/zones", "gcp/scopes", "gcp/serviceAccount", "pipelines/autorestartPreempted", "all"])
			args, value = parser.parse_known_args(args=unknown)

			if args.parameter == "all":
				config = PipelinesConfig(first_time=True)

			else:
				config = PipelinesConfig()
				section, option = args.parameter.split('/')
				try:
					config.update(section, option, value)
				except ValueError as e:
					print "ERROR: couldn't update the configuration : {reason}".format(reason=e)
					exit(-1)

if __name__ == "__main__":
	parser = argparse.ArgumentParser(description="")
	subparsers = parser.add_subparsers(dest="subcommand")

	deploy = subparsers.add_parser("deploy")

	scheduler = subparsers.add_parser("scheduler")
	scheduler.add_argument("schedulerOperation", choices=["start", "stop"])

	submit = subparsers.add_parser("submit", conflict_handler="resolve")
	submit.add_argument("--pipelineName", required=True)
	submit.add_argument("--imageName", required=True)
	submitGrp = submit.add_mutually_exclusive_group(required=True)
	submitGrp.add_argument("--scriptUrl")
	submitGrp.add_argument("--cmd")
	submit.add_argument("--logsPath", required=True)
	submit.add_argument("--diskSize", required=False, default=None)
	submit.add_argument("--diskType", required=False, default=None)
	submit.add_argument("--cores", required=False, default=1)
	submit.add_argument("--mem", required=False, default=1)
	submit.add_argument("--inputs", required=False, default=None)
	submit.add_argument("--outputs", required=False, default=None)
	submit.add_argument("--env", required=False, default=None)
	submit.add_argument("--tag", required=False, default=None)
	submit.add_argument("--preemptible", action="store_true", default=False)

	stop = subparsers.add_parser("delete")
	stopGrp = stop.add_mutually_exclusive_group()
	stopGrp.add_argument("--jobIds")
	stopGrp.add_argument("--pipeline")

	restart = subparsers.add_parser("restart")
	restartGrp = restart.add_mutually_exclusive_group()
	restartGrp.add_argument("--jobIds")
	restartGrp.add_argument("--preempted", action="store_true")

	logs = subparsers.add_parser("logs")
	logs.add_argument("jobId")
	logs.add_argument("--stderr", action="store_true")
	logs.add_argument("--stdout", action="store_true")

	lst = subparsers.add_parser("list")
	lst.add_argument("entity", choices=["jobs", "schemas"])
	
	config = subparsers.add_parser("config")
	config.add_argument("configOperation", choices=["set"])
	
	args, unknown = parser.parse_known_args()
	main(args, unknown)
